{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AfshinRezakhani/Thesis1/blob/main/GAN_GRU_UCIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da4u1CWRSVPL",
        "outputId": "f374954a-f427-4e9e-d14d-8407181d5821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª: Index(['A', 'B', 'C'], dtype='object')\n",
            "ğŸ”¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ target: ['A']\n",
            "ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø±Ù‡Ø§ÛŒ ÛŒÚ©ØªØ§ Ø¯Ø± target: (array([0, 1]), array([ 1897, 69709]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Discriminator Loss: 0.6915, Generator Loss: 0.6932\n",
            "Epoch 100: Discriminator Loss: 0.6922, Generator Loss: 0.6912\n",
            "Epoch 200: Discriminator Loss: 0.6954, Generator Loss: 0.6833\n",
            "Epoch 300: Discriminator Loss: 0.6977, Generator Loss: 0.6782\n",
            "Epoch 400: Discriminator Loss: 0.6990, Generator Loss: 0.6755\n",
            "Epoch 500: Discriminator Loss: 0.6997, Generator Loss: 0.6739\n",
            "Epoch 600: Discriminator Loss: 0.7002, Generator Loss: 0.6727\n",
            "Epoch 700: Discriminator Loss: 0.7006, Generator Loss: 0.6718\n",
            "Epoch 800: Discriminator Loss: 0.7010, Generator Loss: 0.6711\n",
            "Epoch 900: Discriminator Loss: 0.7013, Generator Loss: 0.6706\n",
            "âœ… Balanced dataset saved as /content/UCIP_balanced_WithGRU.csv\n",
            "\u001b[1m28/28\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "ğŸ”¹ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„: (array([1]), array([14322]))\n",
            "âœ… Accuracy: 0.9737\n",
            "âœ… F1-Score: 0.9867\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Reshape, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª (UCI ÛŒØ§ UCIP)\n",
        "file_path = \"/content/UCIP.csv\"  # ÛŒØ§ \"/content/UCI.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ùˆ ÛŒØ§ÙØªÙ† target\n",
        "print(\"ğŸ”¹ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø³Øª:\", df.columns)\n",
        "\n",
        "# Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† Ù…Ù†Ø§Ø³Ø¨ (Ø¨Ø§ Ú©Ù…ØªØ± Ø§Ø² 10 Ù…Ù‚Ø¯Ø§Ø± ÛŒÚ©ØªØ§)\n",
        "potential_targets = [col for col in df.columns if df[col].nunique() <= 10]\n",
        "print(\"ğŸ”¹ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ target:\", potential_targets)\n",
        "\n",
        "# ØªÙ†Ø¸ÛŒÙ… Ø³ØªÙˆÙ† Ù‡Ø¯Ù\n",
        "target_column = \"A\"  # Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶\n",
        "if target_column not in df.columns or df[target_column].nunique() > 10:\n",
        "    print(\"âŒ Ø³ØªÙˆÙ† 'A' Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª! Ø¯Ø± Ø­Ø§Ù„ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ...\")\n",
        "    target_column = potential_targets[0]  # Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ù…Ù†Ø§Ø³Ø¨\n",
        "    print(f\"âœ… Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ø¬Ø¯ÛŒØ¯: {target_column}\")\n",
        "\n",
        "# 3. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "features = df.drop(columns=[target_column]).values\n",
        "target = df[target_column].values\n",
        "\n",
        "# Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ ÛŒÚ©ØªØ§ Ø¯Ø± `target`\n",
        "print(\"ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø±Ù‡Ø§ÛŒ ÛŒÚ©ØªØ§ Ø¯Ø± target:\", np.unique(target, return_counts=True))\n",
        "\n",
        "# 4. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# 5. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ³Øª\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 6. ØªØºÛŒÛŒØ± Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ GRU\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# 7. Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ùˆ Ú©Ù„Ø§Ø³ Ø¯Ø± `y_train` ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯\n",
        "assert len(np.unique(y_train)) > 1, \"âŒ Ø®Ø·Ø§: ÙÙ‚Ø· ÛŒÚ© Ú©Ù„Ø§Ø³ Ø¯Ø± y_train ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯!\"\n",
        "\n",
        "# 8. ØªØ¹Ø±ÛŒÙ Generator\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Input(shape=(1, X_train.shape[2])),\n",
        "        GRU(128, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        GRU(64, return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        GRU(32, return_sequences=False),\n",
        "        Dense(X_train.shape[2]),  # Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§\n",
        "        Reshape((1, X_train.shape[2]))  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ (samples, 1, features)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 9. ØªØ¹Ø±ÛŒÙ Discriminator\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Input(shape=(1, X_train.shape[2])),\n",
        "        GRU(128, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        GRU(64, return_sequences=False),\n",
        "        Dense(1, activation='sigmoid')  # Ø®Ø±ÙˆØ¬ÛŒ 0 ÛŒØ§ 1\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 10. ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„ GAN\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Ø¹Ø¯Ù… Ø¢Ù…ÙˆØ²Ø´ Discriminator Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ GAN\n",
        "    model = Sequential([generator, discriminator])\n",
        "    return model\n",
        "\n",
        "# 11. Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0002, 0.5), metrics=[\"accuracy\"])\n",
        "\n",
        "generator = build_generator()\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# 12. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ GAN\n",
        "epochs = 1000  # Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ epochs Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ø¨ÛŒØ´ØªØ±\n",
        "batch_size = 128  # Ø§ÙØ²Ø§ÛŒØ´ batch_size Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù†ÙˆØ³Ø§Ù†Ø§Øª Ø²ÛŒØ§Ø¯\n",
        "half_batch = batch_size // 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¹Ù„ÛŒ\n",
        "    noise = np.random.normal(0, 1, (half_batch, 1, X_train.shape[2]))\n",
        "    generated_data = generator.predict(noise, verbose=0)\n",
        "\n",
        "    # Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ØªØµØ§Ø¯ÙÛŒ\n",
        "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
        "    real_data = X_train[idx]\n",
        "\n",
        "    # Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø¬Ø¹Ù„ÛŒ (Label Smoothing)\n",
        "    real_labels = np.random.uniform(0.8, 1.0, (half_batch, 1))\n",
        "    fake_labels = np.random.uniform(0.0, 0.2, (half_batch, 1))\n",
        "\n",
        "    # ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø¬Ø¹Ù„ÛŒ\n",
        "    X_combined = np.concatenate((real_data, generated_data))\n",
        "    y_combined = np.concatenate((real_labels, fake_labels))\n",
        "\n",
        "    # Ø¢Ù…ÙˆØ²Ø´ Discriminator Ø¯Ø± Ù‡Ø± 2 epoch\n",
        "    if epoch % 2 == 0:\n",
        "        d_loss = discriminator.train_on_batch(X_combined, y_combined)\n",
        "\n",
        "    # Ø¢Ù…ÙˆØ²Ø´ Generator Ø¨Ø±Ø§ÛŒ ÙØ±ÛŒØ¨ Ø¯Ø§Ø¯Ù† Discriminator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 1, X_train.shape[2]))\n",
        "    y_mislabeled = np.ones((batch_size, 1))\n",
        "\n",
        "    g_loss = gan.train_on_batch(noise, y_mislabeled)\n",
        "\n",
        "    d_loss = float(d_loss[0]) if isinstance(d_loss, list) else float(d_loss)\n",
        "    g_loss = float(g_loss[0]) if isinstance(g_loss, list) else float(g_loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Discriminator Loss: {d_loss:.4f}, Generator Loss: {g_loss:.4f}\")\n",
        "\n",
        "# 13. ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³ Ø§Ù‚Ù„ÛŒØª\n",
        "minority_class = 0 if np.bincount(y_train)[0] < np.bincount(y_train)[1] else 1\n",
        "num_samples_to_generate = abs(np.bincount(y_train)[0] - np.bincount(y_train)[1])\n",
        "\n",
        "noise = np.random.normal(0, 1, (num_samples_to_generate, 1, X_train.shape[2]))\n",
        "generated_samples = generator.predict(noise, verbose=0)\n",
        "\n",
        "# Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡\n",
        "synthetic_labels = np.full((num_samples_to_generate,), minority_class)\n",
        "\n",
        "# 14. ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§\n",
        "X_balanced = np.concatenate((X_train.squeeze(), generated_samples.squeeze()))\n",
        "y_balanced = np.concatenate((y_train, synthetic_labels))\n",
        "\n",
        "# 15. Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§Ù„Ø§Ù†Ø³â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ø³ØªÙˆÙ† Ø§Ø³Øª\n",
        "df_balanced = pd.DataFrame(X_balanced)\n",
        "df_balanced.insert(0, target_column, y_balanced)  # Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù† Ø³ØªÙˆÙ† Ù‡Ø¯Ù Ø¯Ø± Ø³ØªÙˆÙ† Ø§ÙˆÙ„\n",
        "\n",
        "balanced_file_path = \"/content/UCIP_balanced_WithGRU.csv\"\n",
        "df_balanced.to_csv(balanced_file_path, index=False)\n",
        "print(f\"âœ… Balanced dataset saved as {balanced_file_path}\")\n",
        "\n",
        "# 16. Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„\n",
        "y_pred_probs = discriminator.predict(X_test, batch_size=512, verbose=1)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"ğŸ”¹ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ø¯Ù„:\", np.unique(y_pred, return_counts=True))\n",
        "\n",
        "# 17. Ù…Ø­Ø§Ø³Ø¨Ù‡ Accuracy Ùˆ F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
        "print(f\"âœ… F1-Score: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNGpqE50MP1W38UKMl9k5O5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}